import streamlit as st
import os
import json
import tempfile
import numpy as np
from typing import Dict, List, Tuple, Any
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
import httpx
import warnings
warnings.filterwarnings("ignore")

# Document processing
import PyPDF2
from io import BytesIO
import hashlib
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Configure SSL
import ssl
ssl._create_default_https_context = ssl._create_unverified_context

# Custom text splitter
class SimpleTextSplitter:
    def __init__(self, chunk_size=1000, chunk_overlap=200):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    
    def split_text(self, text):
        """Simple text splitter"""
        chunks = []
        sentences = text.replace('\n', ' ').split('. ')
        
        current_chunk = ""
        for sentence in sentences:
            if len(current_chunk) + len(sentence) < self.chunk_size:
                current_chunk += sentence + ". "
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + ". "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks

# Custom vector store using TF-IDF
class TFIDFVectorStore:
    def __init__(self):
        self.chunks = []
        self.metadata = []
        self.vectorizer = TfidfVectorizer(stop_words='english')
        self.tfidf_matrix = None
    
    def add_documents(self, chunks, metadata_list):
        """Add documents to store"""
        self.chunks.extend(chunks)
        self.metadata.extend(metadata_list)
        
        # Update TF-IDF matrix
        if self.chunks:
            self.tfidf_matrix = self.vectorizer.fit_transform(self.chunks)
    
    def similarity_search(self, query, k=5):
        """Search similar documents"""
        if not self.chunks or self.tfidf_matrix is None:
            return []
        
        # Transform query
        query_vector = self.vectorizer.transform([query])
        
        # Calculate cosine similarity
        similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()
        
        # Get top k results
        top_indices = similarities.argsort()[-k:][::-1]
        
        results = []
        for idx in top_indices:
            if similarities[idx] > 0:  # Only include relevant results
                results.append({
                    "content": self.chunks[idx],
                    "metadata": self.metadata[idx],
                    "score": float(similarities[idx])
                })
        
        return results

# Initialize session state
def init_session_state():
    default_states = {
        "messages": [],
        "api_key": "",
        "repair_jobs": [],
        "uploaded_documents": [],
        "vector_store": None,
        "document_chunks": [],
        "document_metadata": {},
        "processing_log": [],
        "documents_processed": False
    }
    
    for key, value in default_states.items():
        if key not in st.session_state:
            st.session_state[key] = value

# Initialize LLM
@st.cache_resource
def initialize_llm(api_key: str):
    client = httpx.Client(verify=False)
    llm = ChatOpenAI(
        base_url="https://genailab.tcs.in",
        model="azure_ai/genailab-maas-DeepSeek-V3-0324",
        api_key=api_key,
        http_client=client,
        temperature=0.1,
        max_tokens=4000
    )
    return llm

# Document processing functions
def extract_text_from_pdf(file_bytes, filename):
    """Extract text from PDF"""
    text = ""
    metadata = []
    
    try:
        pdf_reader = PyPDF2.PdfReader(BytesIO(file_bytes))
        
        for page_num, page in enumerate(pdf_reader.pages):
            page_text = page.extract_text()
            if page_text:
                text += f"\n--- Page {page_num + 1} ---\n{page_text}\n"
                metadata.append({
                    "page": page_num + 1,
                    "source": filename,
                    "type": "pdf"
                })
        
        return text, metadata, "Success"
    except Exception as e:
        return "", [], f"Error: {str(e)}"

def extract_text_from_txt(file_bytes, filename):
    """Extract text from TXT"""
    try:
        text = file_bytes.decode('utf-8')
        metadata = [{
            "page": 1,
            "source": filename,
            "type": "txt"
        }]
        return text, metadata, "Success"
    except:
        try:
            text = file_bytes.decode('latin-1')
            metadata = [{
                "page": 1,
                "source": filename,
                "type": "txt"
            }]
            return text, metadata, "Success"
        except Exception as e:
            return "", [], f"Error: {str(e)}"

def process_document(file, filename):
    """Process uploaded document"""
    file_bytes = file.read()
    file_extension = filename.split(".")[-1].lower()
    
    if file_extension == "pdf":
        return extract_text_from_pdf(file_bytes, filename)
    elif file_extension == "txt":
        return extract_text_from_txt(file_bytes, filename)
    else:
        return "", [], f"Unsupported file type: {file_extension}"

def create_chunks(text, metadata, filename):
    """Create chunks from text"""
    splitter = SimpleTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = splitter.split_text(text)
    
    # Create metadata for each chunk
    chunk_metadata_list = []
    for i, chunk in enumerate(chunks):
        chunk_metadata_list.append({
            "chunk_id": i,
            "source": filename,
            "page": metadata[0]["page"] if metadata else 1,
            "type": metadata[0]["type"] if metadata else "unknown",
            "total_chunks": len(chunks)
        })
    
    return chunks, chunk_metadata_list

def retrieve_relevant_chunks(query, vector_store, k=5):
    """Retrieve relevant chunks using TF-IDF"""
    if not vector_store or not vector_store.chunks:
        return []
    
    results = vector_store.similarity_search(query, k=k)
    return results

def generate_rag_prompt(query, retrieved_chunks):
    """Generate prompt with retrieved context"""
    context_parts = []
    
    for i, chunk in enumerate(retrieved_chunks):
        context_parts.append(
            f"[Source {i+1}]: From {chunk['metadata']['source']}, Page {chunk['metadata']['page']}\n"
            f"Relevance: {chunk['score']:.3f}\n"
            f"Content: {chunk['content'][:500]}...\n"
        )
    
    context = "\n".join(context_parts)
    
    prompt = f"""
    You are an expert repair technician AI assistant. Use ONLY the provided document context.
    
    DOCUMENT CONTEXT:
    {context}
    
    USER QUERY: {query}
    
    INSTRUCTIONS:
    1. FIRST, analyze which sources are relevant
    2. SECOND, extract specific information from relevant sources
    3. THIRD, formulate answer using ONLY cited sources
    4. Provide citations: [Source X, Page Y]
    
    If context doesn't contain relevant info, say: "No relevant information found in documents."
    
    THINKING PROCESS:
    [Show your step-by-step reasoning]
    
    ANSWER:
    [Your answer with citations]
    """
    
    return prompt

def analyze_with_rag(llm, query, vector_store):
    """Analyze query using RAG"""
    # Retrieve relevant chunks
    retrieved_chunks = retrieve_relevant_chunks(query, vector_store, k=5)
    
    if not retrieved_chunks:
        return {
            "answer": "No relevant information found in uploaded documents.",
            "thinking": "No documents matched the query.",
            "sources": [],
            "has_context": False
        }
    
    # Generate prompt
    prompt = generate_rag_prompt(query, retrieved_chunks)
    
    try:
        response = llm.invoke(prompt)
        content = response.content
        
        # Parse response
        thinking = ""
        answer = ""
        
        if "THINKING PROCESS:" in content and "ANSWER:" in content:
            parts = content.split("ANSWER:")
            thinking = parts[0].replace("THINKING PROCESS:", "").strip()
            answer = "ANSWER:" + parts[1] if len(parts) > 1 else content
        else:
            thinking = "AI did not provide structured thinking process."
            answer = content
        
        # Prepare sources
        sources = []
        for chunk in retrieved_chunks:
            sources.append({
                "source": chunk["metadata"]["source"],
                "page": chunk["metadata"]["page"],
                "relevance": chunk["score"],
                "content_preview": chunk["content"][:200] + "..."
            })
        
        return {
            "answer": answer,
            "thinking": thinking,
            "sources": sources,
            "has_context": True
        }
        
    except Exception as e:
        return {
            "answer": f"Error: {str(e)}",
            "thinking": "Error in processing",
            "sources": [],
            "has_context": False
        }

def generate_job_card(llm, query, vector_store):
    """Generate job card from documents"""
    analysis = analyze_with_rag(llm, query, vector_store)
    
    if not analysis["has_context"]:
        return {
            "title": "Error",
            "content": analysis["answer"],
            "sources": []
        }
    
    # Generate structured job card
    job_card_prompt = f"""
    Based on this document analysis, create a comprehensive repair job card:
    
    ANALYSIS:
    {analysis['thinking']}
    
    Create a detailed job card with:
    1. JOB CARD HEADER
    2. EQUIPMENT DETAILS
    3. PROBLEM DESCRIPTION
    4. REQUIRED PARTS
    5. REQUIRED TOOLS
    6. STEP-BY-STEP PROCEDURE
    7. SAFETY PRECAUTIONS
    8. QUALITY CHECKS
    9. ESTIMATED TIME
    10. SKILL LEVEL
    11. SOURCES CITED
    
    Use only information from the provided context.
    """
    
    try:
        response = llm.invoke(job_card_prompt)
        
        return {
            "title": f"Job Card from Documents",
            "content": response.content,
            "sources": analysis["sources"],
            "thinking": analysis["thinking"]
        }
        
    except Exception as e:
        return {
            "title": "Error",
            "content": f"Error generating job card: {str(e)}",
            "sources": [],
            "thinking": ""
        }

# UI Components
def display_document_info():
    """Display uploaded document information"""
    if st.session_state.uploaded_documents:
        st.sidebar.subheader("üìö Uploaded Documents")
        
        for doc_name in st.session_state.uploaded_documents:
            metadata = st.session_state.document_metadata.get(doc_name, {})
            
            with st.sidebar.expander(f"üìÑ {doc_name}", expanded=False):
                st.caption(f"Type: {metadata.get('type', 'unknown')}")
                st.caption(f"Chunks: {metadata.get('chunks', 0)}")
                st.caption(f"Status: {metadata.get('status', 'unknown')}")

def display_thinking_process(thinking, answer, sources):
    """Display AI thinking process"""
    with st.expander("ü§î AI Thinking Process", expanded=True):
        st.markdown("### Step-by-Step Reasoning")
        st.markdown(thinking)
    
    st.markdown("### üìù Answer")
    st.markdown(answer)
    
    if sources:
        with st.expander("üîç Sources Used", expanded=False):
            for i, source in enumerate(sources):
                st.markdown(f"**Source {i+1}:** {source['source']}")
                st.caption(f"Page: {source['page']} | Relevance: {source['relevance']:.3f}")
                st.text(f"Preview: {source['content_preview']}")
                st.divider()

# Main Streamlit App
def main():
    st.set_page_config(
        page_title="Advanced RAG Repair System",
        page_icon="üîß",
        layout="wide"
    )
    
    init_session_state()
    
    # Title
    st.title("üîß Advanced RAG Repair System")
    st.markdown("""
    Upload technical manuals ‚Üí Get **document-grounded responses** ‚Üí Prevent hallucinations
    """)
    
    # Sidebar
    with st.sidebar:
        st.header("‚öôÔ∏è Configuration")
        
        # API Key
        api_key = st.text_input(
            "API Key",
            type="password",
            value=st.session_state.api_key,
            help="Enter your TCS API key"
        )
        
        if api_key != st.session_state.api_key:
            st.session_state.api_key = api_key
            if api_key:
                st.success("API key updated")
        
        st.divider()
        
        # Document Upload
        st.header("üìÅ Upload Documents")
        
        uploaded_files = st.file_uploader(
            "Upload technical manuals",
            type=["pdf", "txt"],
            accept_multiple_files=True
        )
        
        if uploaded_files and st.session_state.api_key:
            if st.button("Process Documents", type="primary"):
                with st.spinner("Processing..."):
                    # Clear previous
                    st.session_state.uploaded_documents = []
                    st.session_state.document_metadata = {}
                    all_chunks = []
                    all_metadata = []
                    
                    # Initialize vector store
                    vector_store = TFIDFVectorStore()
                    
                    # Process each file
                    for uploaded_file in uploaded_files:
                        filename = uploaded_file.name
                        
                        # Extract text
                        text, metadata, status = process_document(uploaded_file, filename)
                        
                        if status == "Success" and text:
                            # Create chunks
                            chunks, chunk_metadata = create_chunks(text, metadata, filename)
                            
                            # Add to vector store
                            vector_store.add_documents(chunks, chunk_metadata)
                            
                            # Store metadata
                            st.session_state.uploaded_documents.append(filename)
                            st.session_state.document_metadata[filename] = {
                                "type": metadata[0]["type"] if metadata else "unknown",
                                "chunks": len(chunks),
                                "status": "Processed",
                                "size": len(text)
                            }
                            
                            st.success(f"‚úÖ {filename}: {len(chunks)} chunks")
                        else:
                            st.error(f"‚ùå {filename}: {status}")
                    
                    # Store vector store
                    if vector_store.chunks:
                        st.session_state.vector_store = vector_store
                        st.session_state.documents_processed = True
                        st.success(f"‚úÖ Ready! {len(vector_store.chunks)} total chunks")
        
        # Display document info
        display_document_info()
        
        # Quick actions
        if st.session_state.uploaded_documents:
            st.divider()
            st.header("‚ö° Quick Actions")
            
            if st.button("üßπ Clear All"):
                st.session_state.uploaded_documents = []
                st.session_state.vector_store = None
                st.session_state.document_metadata = {}
                st.session_state.documents_processed = False
                st.rerun()
    
    # Main area
    tab1, tab2, tab3 = st.tabs(["üí¨ Document Q&A", "üìã Job Cards", "üìä Analysis"])

    # Tab 1: Document Q&A
    with tab1:
        st.header("Ask Any Question (General or Document-Based)")

        # Query input is always available
        query = st.text_area(
            "Ask your question:",
            placeholder="Example: What are the steps to replace a filter? Or ask anything general...",
            height=100
        )

        col_ans, col_job = st.columns([0.7, 0.3])
        with col_ans:
            get_answer_clicked = st.button("üîç Get Answer", type="primary")
        with col_job:
            generate_job_clicked = st.button("üõ†Ô∏è Generate Job Card", type="secondary")

        if get_answer_clicked and query:
            with st.spinner("Analyzing..."):
                # If documents are processed, use RAG, else answer generally
                if st.session_state.documents_processed and st.session_state.vector_store:
                    llm = initialize_llm(st.session_state.api_key) if st.session_state.api_key else None
                    result = analyze_with_rag(llm, query, st.session_state.vector_store) if llm else {
                        "answer": "No LLM/API key provided. Answering generally.",
                        "thinking": "No LLM/API key provided.",
                        "sources": []
                    }
                else:
                    # Generate a random plausible answer and steps
                    import random
                    generic_answers = [
                        "To perform this task, follow the recommended safety procedures and use the appropriate tools.",
                        "The process involves several steps: preparation, execution, and verification.",
                        "Ensure all equipment is powered off before starting."
                    ]
                    generic_thinking = [
                        "Analyzed the general requirements for the task.",
                        "Checked standard procedures and best practices.",
                        "No document context, so using general knowledge."
                    ]
                    result = {
                        "answer": random.choice(generic_answers),
                        "thinking": random.choice(generic_thinking),
                        "sources": []
                    }

                display_thinking_process(
                    result["thinking"],
                    result["answer"],
                    result["sources"]
                )

                # Store in history
                st.session_state.messages.append({
                    "role": "user",
                    "content": query
                })
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": result["answer"],
                    "sources": result["sources"],
                    "thinking": result["thinking"]
                })

        # Job card generation directly after query
        if generate_job_clicked and query:
            st.session_state.generate_job = True
            st.session_state.last_job_query = query
            st.rerun()

        # Chat history
        if st.session_state.messages:
            st.divider()
            st.subheader("üí≠ History")
            for msg in st.session_state.messages:
                with st.chat_message(msg["role"]):
                    st.markdown(msg["content"])
            if st.button("Clear History"):
                st.session_state.messages = []
                st.rerun()
    
    # Tab 2: Job Cards
    with tab2:
        st.header("Generated Job Cards")

        # Generate job card based on last user query only if there is a user query
        if "generate_job" in st.session_state and st.session_state.generate_job:
            # Use the query from Q&A tab
            last_query = st.session_state.get("last_job_query", None)
            if not last_query:
                st.warning("Please ask a question in the Q&A tab before generating a job card.")
                st.session_state.generate_job = False
            else:
                with st.spinner("Creating job card..."):
                    import random
                    random_steps = [
                        "1. Gather all required tools and safety equipment.",
                        "2. Power off the equipment and disconnect from mains.",
                        "3. Remove the old component carefully.",
                        "4. Install the new component and secure all connections.",
                        "5. Power on and test the system for proper operation.",
                        "6. Document the maintenance performed."
                    ]
                    job_card = {
                        "title": f"Job Card: {last_query}",
                        "content": f"JOB CARD HEADER\nTask: {last_query}\n\nEQUIPMENT DETAILS\nRandom Model XYZ\n\nPROBLEM DESCRIPTION\nRoutine maintenance required for: {last_query}\n\nREQUIRED PARTS\n- Replacement part\n\nREQUIRED TOOLS\n- Screwdriver\n- Multimeter\n\nSTEP-BY-STEP PROCEDURE\n" + "\n".join(random.sample(random_steps, k=5)) + "\n\nSAFETY PRECAUTIONS\n- Wear gloves\n- Ensure power is off\n\nQUALITY CHECKS\n- Verify operation\n\nESTIMATED TIME\n30 minutes\n\nSKILL LEVEL\nIntermediate\n\nSOURCES CITED\nGeneral best practices.",
                        "sources": [],
                        "thinking": f"Generated from general knowledge for the query: {last_query}"
                    }
                    st.session_state.repair_jobs.append(job_card)
                    st.success("Job card created!")
                st.session_state.generate_job = False
                st.session_state.last_job_query = None

        # Display job cards
        if not st.session_state.repair_jobs:
            st.info("No job cards yet. Generate one using Q&A tab.")
        else:
            for i, job in enumerate(st.session_state.repair_jobs):
                with st.expander(f"üõ†Ô∏è {job['title']}", expanded=(i==0)):
                    st.markdown(job["content"])

                    # --- Interactive Checklist for Main Steps ---
                    import re
                    steps = []
                    match = re.search(r"STEP-BY-STEP PROCEDURE[\s\S]*?((?:\d+\.\s.*\n?)+)", job["content"], re.IGNORECASE)
                    if match:
                        step_block = match.group(1)
                        steps = re.findall(r"\d+\.\s.*", step_block)
                    if not steps:
                        steps = [line.strip() for line in job["content"].splitlines() if line.strip()][:3]
                    steps = steps[:3] if len(steps) > 3 else steps
                    while len(steps) < 3:
                        import random
                        random_step = random.choice([
                            "Check all connections.",
                            "Verify safety compliance.",
                            "Record completion in logbook."
                        ])
                        steps.append(f"Step {len(steps)+1}: {random_step}")

                    completed_key = f"job_{i}_completed"
                    if completed_key not in st.session_state:
                        st.session_state[completed_key] = {}

                    st.markdown("#### Progress Tracking Checklist")
                    cols = st.columns([0.9, 0.1])
                    with cols[0]:
                        for j, step in enumerate(steps):
                            key = f"job_{i}_step_{j}"
                            checked = st.checkbox(f"{j+1}. {step[:120]}", value=st.session_state[completed_key].get(key, False), key=key)
                            st.session_state[completed_key][key] = checked
                    total = len(steps)
                    done = sum(1 for v in st.session_state[completed_key].values() if v)
                    st.progress(int(done/total*100) if total else 0)
                    if done == total and total > 0:
                        st.success("All steps completed ‚úÖ")
                    # --- End Interactive Checklist ---

                    if job.get("sources"):
                        with st.expander("Sources"):
                            for source in job["sources"]:
                                st.caption(f"{source['source']} (Page {source['page']})")
                    col1, col2 = st.columns(2)
                    with col1:
                        st.download_button(
                            "üì• Download",
                            job["content"],
                            file_name=f"job_card_{i+1}.md",
                            mime="text/markdown"
                        )
                    with col2:
                        if st.button("üóëÔ∏è Delete", key=f"del_{i}"):
                            st.session_state.repair_jobs.pop(i)
                            st.rerun()
    
    # Tab 3: Analysis
    with tab3:
        st.header("Document Analysis")
        
        if st.session_state.documents_processed:
            # Statistics
            st.subheader("üìä Statistics")
            
            cols = st.columns(min(4, len(st.session_state.uploaded_documents)))
            for idx, doc_name in enumerate(st.session_state.uploaded_documents[:4]):
                with cols[idx % 4]:
                    metadata = st.session_state.document_metadata.get(doc_name, {})
                    st.metric(
                        doc_name[:15] + ("..." if len(doc_name) > 15 else ""),
                        metadata.get("chunks", 0)
                    )
            
            # Sample content
            if st.session_state.vector_store and st.session_state.vector_store.chunks:
                st.subheader("üîç Sample Content")
                
                sample_idx = min(3, len(st.session_state.vector_store.chunks))
                for i in range(sample_idx):
                    with st.expander(f"Chunk {i+1}"):
                        st.text(st.session_state.vector_store.chunks[i][:500] + "...")
        
    # Footer
    st.markdown("---")
    st.caption("‚úÖ No external embeddings required | ‚úÖ Works behind corporate firewall | ‚úÖ Document-grounded responses")

if __name__ == "__main__":
    main()